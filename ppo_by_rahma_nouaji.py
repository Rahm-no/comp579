# -*- coding: utf-8 -*-
"""PPO by Rahma NOUAJI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BDl0kuCfyxxwb_XlgX_I2AqOXhI0hVRt

# PPO by Rahma Nouaji

## Environment setup and installation of stable baseline3
"""

!apt-get update && apt-get install swig cmake
!pip install box2d-py
!pip install "stable-baselines3[extra]>=2.0.0a4"

!pip3 install gym[box2d]

import os

import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt

from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.results_plotter import load_results, ts2xy
from stable_baselines3.common.noise import NormalActionNoise
from stable_baselines3.common.callbacks import BaseCallback

class SaveOnBestTrainingRewardCallback(BaseCallback):


    def __init__(self, check_freq: int, log_dir: str, verbose=1):
        super().__init__(verbose)
        self.check_freq = check_freq
        self.log_dir = log_dir
        self.save_path = os.path.join(log_dir, "best_model")
        self.best_mean_reward = -np.inf

    def _init_callback(self) -> None:
        # Create folder if needed
        if self.save_path is not None:
            os.makedirs(self.save_path, exist_ok=True)

    def _on_step(self) -> bool:
        if self.n_calls % self.check_freq == 0:

            # Retrieve training reward
            x, y = ts2xy(load_results(self.log_dir), "timesteps")
            if len(x) > 0:
                # Mean training reward over the last 100 episodes
                mean_reward = np.mean(y[-100:])
                if self.verbose > 0:
                    print(f"Num timesteps: {self.num_timesteps}")
                    print(
                        f"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}"
                    )

                # New best model, you could save the agent here
                if mean_reward > self.best_mean_reward:
                    self.best_mean_reward = mean_reward
                    # Example for saving best model
                    if self.verbose > 0:
                        print(f"Saving new best model to {self.save_path}.zip")
                    self.model.save(self.save_path)

        return True

"""## Training PPO algorithm with default hyperparameters

####  Default hyperparameters batch size 64 , learning rate .0003 and value function coefficient 0.5
"""

# Create log dir
log_dir64 = "mylog64/"
os.makedirs(log_dir64, exist_ok=True)
# Create and wrap the environment
env = gym.make("LunarLanderContinuous-v2")
# Logs will be saved in log_dir/monitor.csv
env = Monitor(env, log_dir64)

callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir64)
# Create RL model
model = PPO('MlpPolicy', env, batch_size=64,verbose=0)
# Train the agent
model.learn(total_timesteps=int(5e5), callback=callback)

def moving_average(values, window):

    weights = np.repeat(1.0, window) / window
    return np.convolve(values, weights, "valid")


def plot_results(log_folder, title="Learning Curve"):

    x, y = ts2xy(load_results(log_folder), "timesteps")
    y = moving_average(y, window=50)
    # Truncate x
    x = x[len(x) - len(y) :]

    fig = plt.figure(title)
    plt.plot(x, y)
    plt.xlabel("Number of Timesteps")
    plt.ylabel("Rewards")
    plt.title(title + " for PPO with default hyperparameters")
    plt.show()

plot_results(log_dir64
             )

"""##### Evaluation"""

model.save("ppo_lunarLander64")
del model # remove to demonstrate saving and loading

model = PPO.load(("ppo_lunarLander64"))

from stable_baselines3.common.evaluation import evaluate_policy
env = gym.make("LunarLanderContinuous-v2")

# evaluate
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)
print(f"Eval reward: {mean_reward} (+/-{std_reward})")

"""## Ablation study

### Batch size

#### Batch size 128( view Tensorboard for more plots)
"""



log_dir128 =  "my_logs128/"

os.makedirs(log_dir128, exist_ok=True)
# Create and wrap the environment
env = gym.make("LunarLanderContinuous-v2")
# Logs will be saved in log_dir/monitor.csv
# env = Monitor(env, log_dirLR)
env = Monitor(env, log_dir128)

callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir128)
# Create RL model
model = PPO('MlpPolicy', env, batch_size=128,verbose=0,tensorboard_log=log_dir128)
# Train the agent
model.learn(total_timesteps=int(5e5), callback=callback)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir='/content/my_logs128'

"""##### Evaluation for batch size 128"""

model.save("ppo_lunarLander128")

del model # remove to demonstrate saving and loading

model = PPO.load(("ppo_lunarLander128"))

from stable_baselines3.common.evaluation import evaluate_policy
env = gym.make("LunarLanderContinuous-v2")

# evaluate
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)
print(f"Eval reward: {mean_reward} (+/-{std_reward})")

from stable_baselines3.common import results_plotter

# Helper from the library
results_plotter.plot_results(
    [log_dir128], 1e5, results_plotter.X_TIMESTEPS, "PPO LunarLander 128"
)

"""#### Batch size 32(view Tensorboard for more plots)"""

log_dir32 = "my_logs_32b/"
os.makedirs(log_dir32, exist_ok=True)

env = gym.make("LunarLanderContinuous-v2")
# Logs will be saved in log_dir/monitor.csv
# env = Monitor(env, log_dirLR)
env = Monitor(env, log_dir32)



callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dir32)
# Create RL model
model = PPO('MlpPolicy', env, batch_size=32,verbose=0,tensorboard_log=log_dir32)
# Train the agent
model.learn(total_timesteps=int(5e5), callback=callback)

from stable_baselines3.common import results_plotter

# Helper from the library
results_plotter.plot_results(
    [log_dir32], 1e5, results_plotter.X_TIMESTEPS, "PPO LunarLander 32"
)

##Find the process for tensorboard and kill it before launching the tensorboard
# !lsof -i :6006
# !kill -9 55481



# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir='/content/my_logs_32b'

"""##### Evaluation"""

model.save("ppo_lunarLander32")

del model # remove to demonstrate saving and loading

model = PPO.load(("ppo_lunarLander32"))

from stable_baselines3.common.evaluation import evaluate_policy

# evaluate
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)
print(f"Eval reward: {mean_reward} (+/-{std_reward})")

"""#### Comparison between the different batch sizes values  on PPO"""

def moving_average(values, window):

    weights = np.repeat(1.0, window) / window
    return np.convolve(values, weights, "valid")


def plot_results(log_folder0,log_folder1,log_folder2, title="Learning Curve for PPO agent with Lunar Lander"):
    x0, y0 = ts2xy(load_results(log_folder0), "timesteps")
    y0 = moving_average(y0, window=50)
        # Truncate x
    x0 = x0[len(x0) - len(y0) :]


    x, y = ts2xy(load_results(log_folder1), "timesteps")
    y = moving_average(y, window=50)
    # Truncate x
    x = x[len(x) - len(y) :]

    x1, y1 = ts2xy(load_results(log_folder2), "timesteps")
    y1 = moving_average(y1, window=50)
    # Truncate x
    x1 = x1[len(x1) - len(y1) :]


    fig = plt.figure(title)
    plt.plot(x, y,'b-', label=' batch_size=128 ')
    plt.plot(x1, y1,'r-',label='batch_size = 64 ')
    plt.plot(x0, y0,'g-',label='batch_size = 32 ')

    plt.legend()
    plt.xlabel("Number of Timesteps")
    plt.ylabel("Rewards")
    plt.title(title)
    plt.show()

plot_results("my_logs_32b/",log_dir128,log_dir64)

"""### Learning rate

#### Linear learning rate schedule with 0.001( view Tensorboard below )
"""

# %rm -r /content/my_logslr

from typing import Callable

def linear_schedule(initial_value: float) -> Callable[[float], float]:

    def func(progress_remaining: float) -> float:

        return progress_remaining * initial_value

    return func

log_dirlr_schedu =  "my_logslr/"

os.makedirs(log_dirlr_schedu, exist_ok=True)
# Create and wrap the environment
env = gym.make("LunarLanderContinuous-v2")
# Logs will be saved in log_dir/monitor.csv
# env = Monitor(env, log_dirLR)
env = Monitor(env, log_dirlr_schedu)

model = PPO("MlpPolicy", env, learning_rate=linear_schedule(0.001), verbose=0,tensorboard_log=log_dirlr_schedu)

callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dirlr_schedu)
# Train the agent
model.learn(total_timesteps=int(5e5), callback=callback)



##Find the process for tensorboard and kill it before launching the tensorboard
# !lsof -i :6006
# !kill -9 56966

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir='/content/my_logslr'

"""##### Evaluation"""

model.save("ppo_lunarLanderlinearlr")

del model # remove to demonstrate saving and loading

model = PPO.load(("ppo_lunarLanderlinearlr"))

from stable_baselines3.common.evaluation import evaluate_policy

# evaluate
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)
print(f"Eval reward: {mean_reward} (+/-{std_reward})")

from stable_baselines3.common import results_plotter

# Helper from the library
results_plotter.plot_results(
    [log_dirlr_schedu], 1e5, results_plotter.X_TIMESTEPS, "PPO LunarLander with linear learning rate schedule"
)

"""#### Learning rate 0.01"""

log_dirlr_001 =  "my_logslr1/"

os.makedirs(log_dirlr_001, exist_ok=True)
# Create and wrap the environment
env = gym.make("LunarLanderContinuous-v2")
# Logs will be saved in log_dir/monitor.csv
# env = Monitor(env, log_dirLR)
env = Monitor(env, log_dirlr_001)

model = PPO("MlpPolicy", env, learning_rate=0.01, verbose=0)

callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=log_dirlr_001)
# Train the agent
model.learn(total_timesteps=int(5e5), callback=callback)

from stable_baselines3.common import results_plotter

# Helper from the library
results_plotter.plot_results(
    [log_dirlr_001], 1e5, results_plotter.X_TIMESTEPS, "PPO LunarLander with  learning rate 0.01"
)



"""##### Evaluation"""

model.save("ppo_lunarLanderlinearlr001")

del model # remove to demonstrate saving and loading

model = PPO.load(("ppo_lunarLanderlinearlr001"))

from stable_baselines3.common.evaluation import evaluate_policy

# evaluate
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)
print(f"Eval reward: {mean_reward} (+/-{std_reward})")

"""#### Comparison between the different learning rates used on PPO"""

def moving_average(values, window):

    weights = np.repeat(1.0, window) / window
    return np.convolve(values, weights, "valid")


def plot_results(log_folder0,log_folder1,log_folder2, title="Learning Curve for PPO agent with Lunar Lander"):
    x0, y0 = ts2xy(load_results(log_folder0), "timesteps")


    x, y = ts2xy(load_results(log_folder1), "timesteps")
    x1, y1 = ts2xy(load_results(log_folder2), "timesteps")

    y = moving_average(y, window=50)
    # Truncate x
    x = x[len(x) - len(y) :]
    y1 = moving_average(y1, window=50)
    # Truncate x
    x1 = x1[len(x1) - len(y1) :]
    y0 = moving_average(y0, window=50)
    # Truncate x
    x0 = x0[len(x0) - len(y0) :]


    fig = plt.figure(title)
    plt.plot(x, y,'b-', label=' Learning rate 0.01 ')
    plt.plot(x1, y1,'r-',label='Learning rate 0.0003 default')
    plt.plot(x0, y0,'g-',label='Linear Learning rate schedule(0.001)')

    plt.legend()
    plt.xlabel("Number of Timesteps")
    plt.ylabel("Rewards")
    plt.title(title)
    plt.show()

plot_results('/content/my_logslr',log_dirlr_001,log_dir64)



"""### Grid Search  using different value function coefficients and batch sizes"""



from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.vec_env import DummyVecEnv
from gym.envs.box2d import LunarLanderContinuous
from stable_baselines3.common.evaluation import evaluate_policy

# Define the hyperparameters and the range of value function coefficients to test
hyperparameters = {
    'n_steps': 2048,
    'ent_coef': 0.01,
    'learning_rate': 0.0003,

    'n_epochs': 10,
    'gamma': 0.99,
    'gae_lambda': 0.95,
    'clip_range': 0.2
}

value_coef_range = [0.5, 1.0, 2.0]
batch_size_range = [32,64,128]
for value_coef in value_coef_range:
  for batch_size in batch_size_range:

    env = gym.make("LunarLanderContinuous-v2")

    # Create the PPO agent with the specified value function coefficient
    model = PPO("MlpPolicy", env, verbose=0, **hyperparameters, batch_size = batch_size,vf_coef=value_coef)

    # Train the agent for 500,000 timesteps and save the checkpoints
    checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./ppo_lunarlander_vf{}'.format(value_coef))
    model.learn(total_timesteps=5e5, callback=checkpoint_callback)

     #Evaluate the agent on 100 episodes and print the mean reward
    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)
    print('Value function coefficient {} and Batch size {}: mean reward = {} , std_reward = {}'.format(value_coef,batch_size, mean_reward,std_reward))



"""## Agent with the best hyperparameters

##### Training the Agent with best hyperparameters
"""

env = gym.make("LunarLanderContinuous-v2")

    # Create the PPO agent with the specified value function coefficient
model = PPO("MlpPolicy", env, verbose=0, learning_rate = 0.0003, batch_size = 64,vf_coef=0.5)

# Train the agent for 500,000 timesteps and save the checkpoints
model.learn(total_timesteps=5e5)

"""##### Evaluation"""

model.save("ppo_lunarLanderhpp")

del model # remove to demonstrate saving and loading

model = PPO.load(("ppo_lunarLanderhpp"))

from stable_baselines3.common.evaluation import evaluate_policy

# evaluate
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)
print(f"Eval reward: {mean_reward} (+/-{std_reward})")



"""##### Rendering the env"""

#remove " > /dev/null 2>&1" to see what is going on under the hood
!pip install gym pyvirtualdisplay > /dev/null 2>&1
!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1

!apt-get update > /dev/null 2>&1
!apt-get install cmake > /dev/null 2>&1
!pip install --upgrade setuptools 2>&1
!pip install ez_setup > /dev/null 2>&1
!pip install gym[atari] > /dev/null 2>&1

# Commented out IPython magic to ensure Python compatibility.
import gym
from gym import logger as gymlogger
from gym.wrappers.record_video import RecordVideo
gymlogger.set_level(40) #error only
import tensorflow as tf
import numpy as np
import random
import matplotlib
import matplotlib.pyplot as plt
# %matplotlib inline
import math
import glob
import io
import base64
from IPython.display import HTML

from IPython import display as ipythondisplay

from pyvirtualdisplay import Display
display = Display(visible=0, size=(1400, 900))
display.start()

"""
Utility functions to enable video recording of gym environment and displaying it
To enable video, just do "env = wrap_env(env)""
"""

def show_video():
  mp4list = glob.glob('video/*.mp4')
  if len(mp4list) > 0:
    mp4 = mp4list[0]
    video = io.open(mp4, 'r+b').read()
    encoded = base64.b64encode(video)
    ipythondisplay.display(HTML(data='''<video alt="test" autoplay
                loop controls style="height: 400px;">
                <source src="data:video/mp4;base64,{0}" type="video/mp4" />
             </video>'''.format(encoded.decode('ascii'))))
  else:
    print("Could not find video")


def wrap_env(env):
  env = RecordVideo(env, './video',  episode_trigger = lambda episode_number: True)
  return env

env = wrap_env(gym.make("LunarLanderContinuous-v2",render_mode="rgb_array"))

obs = env.reset()

while True:

    env.render()

    action, _states = model.predict(obs)


    obs, reward, done, info= env.step(action)


    if done:
      break;

env.close()
show_video()

